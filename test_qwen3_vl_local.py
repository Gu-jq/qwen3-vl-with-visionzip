#!/usr/bin/env python3
"""
Qwen3-VL æµ‹è¯•è„šæœ¬ - æ”¯æŒå›¾ç”Ÿæ–‡å’Œè§†è§‰é—®ç­”
- å›¾ç”Ÿæ–‡æ¨¡å¼ï¼š--image <path> ï¼ˆä¸æä¾›é—®é¢˜ï¼‰
- VQAæ¨¡å¼ï¼š--image <path> --question <text>
- æ”¯æŒVisionZipå‹ç¼©ï¼š--use-visionzip --dominant-ratio 0.15 --contextual-ratio 0.05

ç”¨æ³•ç¤ºä¾‹ï¼š
1. å›¾ç”Ÿæ–‡: python test_qwen3_vl_local.py --image /path/to/image.jpg
2. VQA: python test_qwen3_vl_local.py --image /path/to/image.jpg --question "What is in this image?"
3. ä½¿ç”¨VisionZip: python test_qwen3_vl_local.py --image /path/to/image.jpg --use-visionzip
"""
import sys
import os
import argparse
import torch
from PIL import Image

import transformers.utils as _transformers_utils

def _noop_auto_docstring(*args, **kwargs):
    # Decorator no-op to bypass transformers auto_docstring issues on Python 3.12
    if len(args) == 1 and callable(args[0]) and not kwargs:
        return args[0]

    def decorator(obj):
        return obj

    return decorator

_transformers_utils.auto_docstring = _noop_auto_docstring

# ç¡®ä¿å·¥ä½œç›®å½•åœ¨ sys.path å‰é¢ï¼Œè¿™æ ·å¯ä»¥ import æœ¬åœ°çš„ qwen3_vl_visionzip æ¨¡å—
sys.path.insert(0, os.path.dirname(__file__) or os.getcwd())

from transformers import AutoProcessor, Qwen3VLForConditionalGeneration as HFQwen3VLForConditionalGeneration

# ä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥æ¨¡å‹ç±»ï¼ˆæ–‡ä»¶ï¼šqwen3_vl_visionzip.pyï¼‰
from qwen3_vl_visionzip import Qwen3VLForConditionalGeneration as LocalQwen3VLForConditionalGeneration


def main():
    parser = argparse.ArgumentParser(description="Qwen3-VL è§†è§‰æµ‹è¯• (å›¾ç”Ÿæ–‡ & VQA)")
    parser.add_argument("--image", required=True, help="å›¾åƒè·¯å¾„")
    parser.add_argument("--question", type=str, default=None, help="é—®é¢˜æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è¿›è¡Œå›¾ç”Ÿæ–‡ï¼‰")
    parser.add_argument("--model", default="Qwen/Qwen3-VL-2B-Instruct", help="HFæ¨¡å‹ID")
    parser.add_argument("--use-visionzip", action="store_true", help="å¯ç”¨VisionZipå‹ç¼©")
    parser.add_argument("--dominant-ratio", type=float, default=0.15, help="ä¸»è¦Tokenä¿ç•™æ¯”ä¾‹")
    parser.add_argument("--contextual-ratio", type=float, default=0.05, help="ä¸Šä¸‹æ–‡Tokenä¿ç•™æ¯”ä¾‹")
    parser.add_argument("--max-tokens", type=int, default=256, help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("=" * 80)
    print("ğŸš€ Qwen3-VL æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“¦ æ¨¡å‹: {args.model}")
    print(f"ğŸ’» è®¾å¤‡: {device}")
    print(f"ğŸ–¼ï¸  å›¾åƒ: {args.image}")
    
    if args.question:
        print(f"â“ æ¨¡å¼: VQA (è§†è§‰é—®ç­”)")
        print(f"   é—®é¢˜: {args.question}")
    else:
        print(f"ğŸ“ æ¨¡å¼: å›¾ç”Ÿæ–‡ (Image Captioning)")
    
    if args.use_visionzip:
        print(f"ğŸ”§ VisionZip: å·²å¯ç”¨")
        print(f"   â”œâ”€ Dominant Ratio: {args.dominant_ratio}")
        print(f"   â”œâ”€ Contextual Ratio: {args.contextual_ratio}")
        print(f"   â””â”€ æ€»ä¿ç•™ç‡: {(args.dominant_ratio + args.contextual_ratio)*100:.0f}%")
    else:
        print(f"ğŸ”§ VisionZip: æœªå¯ç”¨ (åŸå§‹æ¨¡å‹)")
    print("=" * 80)

    # åŠ è½½ processorï¼ˆç”¨äºå›¾åƒé¢„å¤„ç†ï¼‰
    print("\nğŸ“¥ åŠ è½½ processor...")
    processor = AutoProcessor.from_pretrained(args.model)

    # ä½¿ç”¨æœ¬åœ°å®šä¹‰çš„æ¨¡å‹ç±»æˆ–åŸå§‹æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    if args.use_visionzip:
        model = LocalQwen3VLForConditionalGeneration.from_pretrained(
            args.model,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        model.to(device)
        model.eval()
        
        model.config.visionzip_dominant_ratio = args.dominant_ratio
        model.config.visionzip_contextual_ratio = args.contextual_ratio
        print("   âœ… VisionZipæ¨¡å‹å·²åŠ è½½")
    else:
        model = HFQwen3VLForConditionalGeneration.from_pretrained(
            args.model,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        model.to(device)
        model.eval()
        print("   âœ… åŸå§‹æ¨¡å‹å·²åŠ è½½")

    # åŠ è½½å›¾åƒ
    if not os.path.exists(args.image):
        print(f"\nâŒ é”™è¯¯ï¼šå›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        return
    
    image = Image.open(args.image).convert("RGB")
    print(f"   å›¾åƒå°ºå¯¸: {image.size[0]}x{image.size[1]}")

    # æ„å»ºæ¶ˆæ¯
    if args.question:
        # VQAæ¨¡å¼
        text_prompt = args.question
    else:
        # å›¾ç”Ÿæ–‡æ¨¡å¼
        text_prompt = "Describe the image."
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": text_prompt},
            ],
        }
    ]
    
    print("\nğŸ”„ å‡†å¤‡è¾“å…¥...")
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt",
    )

    pixel_values = inputs.get("pixel_values")
    if pixel_values is None:
        raise RuntimeError("processor did not return 'pixel_values'. Check processor/model compatibility.")
    image_grid_thw = inputs.get("image_grid_thw")
    if image_grid_thw is None:
        raise RuntimeError("processor did not return 'image_grid_thw'. Make sure the processor is compatible with the model.")
    pixel_values = pixel_values.to(device)
    image_grid_thw = image_grid_thw.to(device)

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs.get("attention_mask")
    if attention_mask is not None:
        attention_mask = attention_mask.to(device)
    input_sequence_length = inputs["input_ids"].shape[1]
    print(f"   è¾“å…¥åºåˆ—é•¿åº¦: {input_sequence_length} tokens")

    # å¦‚æœä½¿ç”¨VisionZipï¼Œå…ˆåšä¸€æ¬¡å‰å‘è·å–å‹ç¼©ç»Ÿè®¡
    if args.use_visionzip:
        print("\nğŸ“Š VisionZip å‹ç¼©åˆ†æ...")
        with torch.no_grad():
            _ = model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                image_grid_thw=image_grid_thw,
                return_dict=True,
            )

        before = getattr(model.model, "_last_visionzip_image_tokens_before", None)
        after = getattr(model.model, "_last_visionzip_image_tokens_after", None)
        image_range = getattr(model.model, "_last_visionzip_image_range", (None, None))
        keep_mask = getattr(model.model, "_last_visionzip_keep_mask", None)
        
        if before is not None and before > 0:
            after_value = after if after is not None else 0
            removed = before - after_value
            kept_pct = (after_value / before) * 100
            print(f"   å‹ç¼©å‰: {before} ä¸ªå›¾åƒtokens")
            print(f"   å‹ç¼©å: {after_value} ä¸ªå›¾åƒtokens")
            print(f"   ç§»é™¤: {removed} tokens ({100-kept_pct:.1f}%)")
            print(f"   å®é™…ä¿ç•™ç‡: {kept_pct:.1f}%")
            
            if image_range[0] is not None and image_range[1] is not None:
                span = image_range[1] - image_range[0] + 1
                print(f"   å›¾åƒå ä½ç¬¦èŒƒå›´: {image_range[0]}-{image_range[1]} (å…±{span} tokens)")
        else:
            print("   âš ï¸  æœªæ£€æµ‹åˆ°å›¾åƒå ä½ç¬¦")
        
        if keep_mask is not None:
            final_len = int(keep_mask.sum().item())
            print(f"   å‹ç¼©ååºåˆ—é•¿åº¦: {final_len} / {input_ids.shape[1]} tokens")

    # ç”Ÿæˆå›ç­”
    print(f"\nğŸ¤– ç”Ÿæˆå›ç­” (æœ€å¤š{args.max_tokens}ä¸ªæ–°tokens)...")

    gen_inputs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,
        pixel_values=pixel_values,
        image_grid_thw=image_grid_thw,
        max_new_tokens=args.max_tokens,
        return_dict_in_generate=True,
        do_sample=False,  # è´ªå©ªè§£ç ä¿è¯å¯å¤ç°æ€§
    )

    with torch.no_grad():
        outputs = model.generate(**gen_inputs)

    trimmed_sequences = [
        seq[input_sequence_length:].tolist() for seq in outputs.sequences
    ]
    decoded = processor.batch_decode(
        trimmed_sequences,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 80)
    print("âœ… ç”Ÿæˆç»“æœ:")
    print("-" * 80)
    print(decoded)
    print("=" * 80)


if __name__ == "__main__":
    main()
