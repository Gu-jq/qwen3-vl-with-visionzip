"""
VQA v2.0 è¯„æµ‹è„šæœ¬ for Qwen3-VL with VisionZip

æ•°æ®é›†å‡†å¤‡:
-----------
1. ä¸‹è½½ VQA v2.0 æ•°æ®é›†: https://visualqa.org/download.html
2. ç»„ç»‡æ–‡ä»¶ç»“æ„å¦‚ä¸‹:
   VQA_v2/
   â”œâ”€â”€ v2_OpenEnded_mscoco_val2014_questions.json
   â”œâ”€â”€ v2_mscoco_val2014_annotations.json
   â”œâ”€â”€ val2014/
   â”‚   â”œâ”€â”€ COCO_val2014_000000000042.jpg
   â”‚   â”œâ”€â”€ COCO_val2014_000000000073.jpg
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ v2_OpenEnded_mscoco_train2014_questions.json  (å¯é€‰ï¼Œç”¨äºè®­ç»ƒé›†è¯„æµ‹)
   â”œâ”€â”€ v2_mscoco_train2014_annotations.json          (å¯é€‰)
   â””â”€â”€ train2014/                                     (å¯é€‰)

3. ä¿®æ”¹ç¬¬ 74 è¡Œçš„ VQA_DATA_DIR ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„

é…ç½®ä¸è¿è¡Œ:
-----------
1. ä¿®æ”¹ç¬¬ 80-84 è¡Œçš„é…ç½®å‚æ•°:
   - CONFIG_NAME: ç»“æœæ–‡ä»¶å‘½å
   - USE_COMPRESSION: True/False (æ˜¯å¦ä½¿ç”¨ VisionZip)
   - COMPRESSION_MODULE: "standard"/"notalign"/"layeradjust"/"mixscore"/"all"
   - DOMINANT_RATIO: ä¸»å¯¼ token ä¿ç•™æ¯”ä¾‹ (å¦‚ 0.15 è¡¨ç¤º 15%)
   - CONTEXTUAL_RATIO: ä¸Šä¸‹æ–‡ token ä¿ç•™æ¯”ä¾‹ (å¦‚ 0.05 è¡¨ç¤º 5%)

2. è¿è¡Œç¤ºä¾‹:
   # Baseline (æ— å‹ç¼©)
   python eval_vqa_v2.py
   
   # VisionZip 20% æ ‡å‡†å‹ç¼©
   # å…ˆä¿®æ”¹é…ç½®: USE_COMPRESSION=True, COMPRESSION_MODULE="standard", 
   #            DOMINANT_RATIO=0.15, CONTEXTUAL_RATIO=0.05
   python eval_vqa_v2.py
   
   # åå°è¿è¡Œ
   nohup python eval_vqa_v2.py > vqa_eval.log 2>&1 &

3. ç»“æœæ–‡ä»¶ä¿å­˜åœ¨ vqa_results/ ç›®å½•
"""
import os
import sys
import json
import time
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from datetime import datetime
from collections import defaultdict

# --- 1. ç¯å¢ƒå‡†å¤‡ä¸ Monkeypatch ---
# é’ˆå¯¹ Python 3.12 ä¿®æ”¹ transformers çš„ docstring è£…é¥°å™¨é—®é¢˜
import transformers.utils as _transformers_utils
def _noop_auto_docstring(*args, **kwargs):
    if len(args) == 1 and callable(args[0]) and not kwargs: return args[0]
    def decorator(obj): return obj
    return decorator
_transformers_utils.auto_docstring = _noop_auto_docstring

# å°† VLMEvalKit åŠ å…¥ path ä»¥ä½¿ç”¨æ ‡å‡† VQA è¯„æµ‹å·¥å…·
VISIONZIP_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(VISIONZIP_DIR, "vlmeval"))

from transformers import Qwen3VLForConditionalGeneration as HFQwen3VLForConditionalGeneration
from transformers import AutoProcessor
from vlmeval.dataset.utils.vqa_eval import process_answer

# --- 2. è¯„æµ‹é…ç½®ï¼ˆå•æ¬¡è¿è¡Œæµ‹è¯•ä¸€ç§é…ç½®ï¼‰---
MODEL_PATH = "Qwen/Qwen3-VL-2B-Instruct"
VQA_DATA_DIR = "/YueYangDi/sparky/LLM/LLM_project/dataset/VQA_v2"
OUTPUT_DIR = os.path.join(VISIONZIP_DIR, "vqa_results")
MAX_SAMPLES = 1000  # æµ‹è¯•æ ·æœ¬æ•°ï¼ŒNone è¡¨ç¤ºå…¨é‡
MAX_NEW_TOKENS = 128

# ğŸ”§ é…ç½®å‚æ•°ï¼ˆæ¯æ¬¡è¿è¡Œä¿®æ”¹è¿™äº›å‚æ•°ï¼‰<--- æ‰‹åŠ¨ä¿®æ”¹è¿™é‡Œ
CONFIG_NAME = "Baseline"           # é…ç½®åç§°ï¼ˆç”¨äºç»“æœæ–‡ä»¶å‘½åï¼‰
USE_COMPRESSION = False            # æ˜¯å¦ä½¿ç”¨ VisionZip å‹ç¼©
COMPRESSION_MODULE = None          # å‹ç¼©æ¨¡å—åç§°ï¼š"standard" / "notalign" / "layeradjust" / "mixscore" / "all"
DOMINANT_RATIO = 1.0               # ä¸»å¯¼ token æ¯”ä¾‹
CONTEXTUAL_RATIO = 0.0             # ä¸Šä¸‹æ–‡ token æ¯”ä¾‹

# --- 3. VQA æ ‡å‡†è¯„æµ‹å‡½æ•° ---
def vqa_accuracy(prediction, ground_truths):
    """
    VQA v2 å®˜æ–¹è¯„åˆ†æ ‡å‡†
    å¦‚æœé¢„æµ‹ç­”æ¡ˆåœ¨10ä¸ªground truthä¸­è‡³å°‘å‡ºç°3æ¬¡ï¼Œå¾—1åˆ†
    å¦åˆ™å¾—åˆ†ä¸º min(matching_count/3, 1.0)
    """
    pred_processed = process_answer(prediction)
    gts_processed = [process_answer(gt) for gt in ground_truths]
    
    accuracies = []
    for i, gt in enumerate(gts_processed):
        other_gts = [gts_processed[j] for j in range(len(gts_processed)) if j != i]
        matching = [g for g in other_gts if g == pred_processed]
        acc = min(1.0, len(matching) / 3.0)
        accuracies.append(acc)
    
    return np.mean(accuracies) if accuracies else 0.0

# --- 4. æ•°æ®åŠ è½½ ---
def load_vqa_data(split="val"):
    """åŠ è½½ VQA v2 æ•°æ®"""
    questions_file = os.path.join(VQA_DATA_DIR, f"v2_OpenEnded_mscoco_{split}2014_questions.json")
    annotations_file = os.path.join(VQA_DATA_DIR, f"v2_mscoco_{split}2014_annotations.json")
    image_dir = os.path.join(VQA_DATA_DIR, f"{split}2014")
    
    print(f"æ­£åœ¨åŠ è½½ VQA v2 æ•°æ®é›†: {split}2014...")
    with open(questions_file, 'r') as f:
        questions_data = json.load(f)
    with open(annotations_file, 'r') as f:
        annotations_data = json.load(f)
    
    # æ„å»º question_id åˆ° annotation çš„æ˜ å°„
    id_to_annotation = {ann['question_id']: ann for ann in annotations_data['annotations']}
    
    samples = []
    for q in questions_data['questions']:
        qid = q['question_id']
        if qid not in id_to_annotation:
            continue
        
        ann = id_to_annotation[qid]
        image_id = q['image_id']
        image_filename = f"COCO_{split}2014_{image_id:012d}.jpg"
        image_path = os.path.join(image_dir, image_filename)
        
        if not os.path.exists(image_path):
            continue
        
        # æå–æ‰€æœ‰ç­”æ¡ˆ
        answers = [a['answer'] for a in ann['answers']]
        
        samples.append({
            'question_id': qid,
            'image_path': image_path,
            'question': q['question'],
            'answers': answers,
            'question_type': ann.get('question_type', 'unknown'),
            'answer_type': ann.get('answer_type', 'unknown'),
        })
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if MAX_SAMPLES is not None:
        samples = samples[:MAX_SAMPLES]
    
    print(f"âœ… åŠ è½½å®Œæˆ: {len(samples)} ä¸ªæ ·æœ¬")
    return samples

# --- 5. æ¨¡å‹æ¨ç†ä¸è¯„æµ‹ ---
def process_vision_info(messages):
    """ä»æ¶ˆæ¯ä¸­æå–å›¾åƒ"""
    image_inputs = []
    for message in messages:
        if isinstance(message["content"], list):
            for content in message["content"]:
                if content.get("type") == "image":
                    image_inputs.append(content["image"])
    return image_inputs if image_inputs else None, None

def evaluate_model(model, processor, samples, device, config_name):
    """å•æ ·æœ¬æ¨ç†æ¨¡å¼"""
    results = []
    timing_stats = []
    compression_stats = []
    
    for sample in tqdm(samples, desc=f"è¯„æµ‹ {config_name}"):
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(sample['image_path']).convert('RGB')
            
            # æ„é€ æ¶ˆæ¯ - æ·»åŠ ç®€çŸ­ç­”æ¡ˆæç¤º
            question_with_instruction = f"{sample['question']} Answer with a single word or short phrase."
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": question_with_instruction},
                    ],
                }
            ]
            
            # å‡†å¤‡è¾“å…¥
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            ).to(device)
            
            # æ¨ç†
            start_time = time.time()
            with torch.no_grad():
                generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)
            inference_time = (time.time() - start_time) * 1000  # ms
            
            # è§£ç 
            output_ids = [
                generated_ids[i][len(inputs.input_ids[i]):]
                for i in range(len(generated_ids))
            ]
            prediction = processor.batch_decode(
                output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            # æ”¶é›†å‹ç¼©ç»Ÿè®¡
            if hasattr(model, '_last_visionzip_image_tokens_before'):
                compression_stats.append({
                    'tokens_before': model._last_visionzip_image_tokens_before,
                    'tokens_after': model._last_visionzip_image_tokens_after
                })
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = vqa_accuracy(prediction, sample['answers'])
            
            results.append({
                'question_id': sample['question_id'],
                'question': sample['question'],
                'prediction': prediction,
                'ground_truths': sample['answers'],
                'accuracy': accuracy,
                'question_type': sample['question_type'],
                'answer_type': sample['answer_type'],
                'inference_time_ms': inference_time
            })
            timing_stats.append(inference_time)
            
        except Exception as e:
            print(f"âš ï¸  æ ·æœ¬ {sample['question_id']} å¤„ç†å¤±è´¥: {e}")
            continue
    
    return results, timing_stats, compression_stats

# --- 6. ç»Ÿè®¡è®¡ç®— ---
def compute_statistics(results, timing_stats, compression_stats, config_name):
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'config_name': config_name,
        'total_samples': len(results),
        'overall_accuracy': np.mean([r['accuracy'] for r in results]) * 100,
        'avg_inference_time_ms': np.mean(timing_stats),
        'std_inference_time_ms': np.std(timing_stats),
    }
    
    # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡
    by_qtype = defaultdict(list)
    for r in results:
        by_qtype[r['question_type']].append(r['accuracy'])
    stats['accuracy_by_question_type'] = {
        qtype: np.mean(accs) * 100 for qtype, accs in by_qtype.items()
    }
    
    # æŒ‰ç­”æ¡ˆç±»å‹ç»Ÿè®¡
    by_atype = defaultdict(list)
    for r in results:
        by_atype[r['answer_type']].append(r['accuracy'])
    stats['accuracy_by_answer_type'] = {
        atype: np.mean(accs) * 100 for atype, accs in by_atype.items()
    }
    
    # å‹ç¼©ç»Ÿè®¡
    if compression_stats:
        stats['avg_tokens_before'] = np.mean([s['tokens_before'] for s in compression_stats])
        stats['avg_tokens_after'] = np.mean([s['tokens_after'] for s in compression_stats])
        stats['compression_ratio'] = stats['avg_tokens_after'] / stats['avg_tokens_before'] * 100
    
    return stats

# --- 7. ç»“æœæ‰“å° ---
def print_results(stats):
    """æ‰“å°ç»“æœ"""
    print("\n" + "="*80)
    print(f"ğŸ“Š VQA v2 è¯„æµ‹ç»“æœ - {stats['config_name']}")
    print("="*80)
    
    print(f"\næ€»ä½“å‡†ç¡®ç‡: {stats['overall_accuracy']:.2f}%")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time_ms']:.1f} Â± {stats['std_inference_time_ms']:.1f} ms")
    
    if 'compression_ratio' in stats:
        print(f"å‹ç¼©æ¯”ä¾‹: {stats['avg_tokens_before']:.1f} â†’ {stats['avg_tokens_after']:.1f} tokens ({stats['compression_ratio']:.1f}%)")
    
    print(f"\næŒ‰é—®é¢˜ç±»å‹å‡†ç¡®ç‡:")
    for qtype, acc in sorted(stats['accuracy_by_question_type'].items()):
        print(f"  - {qtype}: {acc:.2f}%")
    
    print(f"\næŒ‰ç­”æ¡ˆç±»å‹å‡†ç¡®ç‡:")
    for atype, acc in sorted(stats['accuracy_by_answer_type'].items()):
        print(f"  - {atype}: {acc:.2f}%")
    
    print("\n" + "="*80)

# --- 8. ä¸»å‡½æ•° ---
def main():
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸš€ å¼€å§‹ VQA v2 è¯„æµ‹")
    samples = load_vqa_data(split="val")
    
    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ processor
    print(f"ğŸ“¥ åŠ è½½ processor: {MODEL_PATH}")
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    processor.tokenizer.padding_side = 'left'  # decoder-only æ¨¡å‹ä½¿ç”¨ left padding
    
    # æ‰“å°å½“å‰é…ç½®
    print(f"\n{'='*80}")
    print(f"ğŸ”§ é…ç½®: {CONFIG_NAME}")
    print(f"   å‹ç¼©æ¨¡å¼: {USE_COMPRESSION}")
    if USE_COMPRESSION:
        print(f"   å‹ç¼©æ¨¡å—: {COMPRESSION_MODULE}")
        print(f"   Tokenæ¯”ä¾‹: dominant={DOMINANT_RATIO}, contextual={CONTEXTUAL_RATIO}")
    print(f"{'='*80}")
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "trust_remote_code": True,
    }
    
    if USE_COMPRESSION:
        # åŠ¨æ€å¯¼å…¥å¯¹åº”çš„å‹ç¼©æ¨¡å—
        module_map = {
            "standard": "qwen3_vl_visionzip",
            "notalign": "qwen3_vl_visionzip_notalign",
            "layeradjust": "qwen3_vl_visionzip_layeradjust",
            "mixscore": "qwen3_vl_visionzip_mixscore",
            "all": "qwen3_vl_visionzip_all",
        }
        
        module_name = module_map.get(COMPRESSION_MODULE, "qwen3_vl_visionzip")
        compression_module = __import__(module_name)
        ModelClass = compression_module.Qwen3VLForConditionalGeneration
        
        model = ModelClass.from_pretrained(MODEL_PATH, **model_kwargs).to(device).eval()
        model.config.visionzip_dominant_ratio = DOMINANT_RATIO
        model.config.visionzip_contextual_ratio = CONTEXTUAL_RATIO
        print(f"âœ… VisionZip å·²é…ç½® ({COMPRESSION_MODULE}): {(DOMINANT_RATIO + CONTEXTUAL_RATIO)*100:.0f}% token ä¿ç•™ç‡")
    else:
        model = HFQwen3VLForConditionalGeneration.from_pretrained(
            MODEL_PATH, **model_kwargs
        ).to(device).eval()
        print("âœ… åŸç”Ÿ Baseline æ¨¡å‹")
    
    # è¯„æµ‹
    results, timing_stats, compression_stats = evaluate_model(
        model, processor, samples, device, CONFIG_NAME
    )
    
    # è®¡ç®—ç»Ÿè®¡
    stats = compute_statistics(results, timing_stats, compression_stats, CONFIG_NAME)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = os.path.join(OUTPUT_DIR, f"vqa_{CONFIG_NAME}_{timestamp}.json")
    with open(results_file, 'w') as f:
        json.dump({'results': results, 'stats': stats}, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
    
    # æ‰“å°ç»“æœ
    print_results(stats)

if __name__ == "__main__":
    main()
